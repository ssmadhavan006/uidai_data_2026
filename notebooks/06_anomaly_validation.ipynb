{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 06: Anomaly Detection Validation\n",
                "\n",
                "Validate anomaly detector using synthetic anomaly injection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Set working directory to project root\n",
                "os.chdir(os.path.dirname(os.path.abspath('__file__')))\n",
                "if os.path.basename(os.getcwd()) == 'notebooks':\n",
                "    os.chdir('..')\n",
                "print(f\"Working dir: {os.getcwd()}\")\n",
                "\n",
                "sys.path.insert(0, 'src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import plotly.express as px\n",
                "\n",
                "from validation_anomaly import (\n",
                "    inject_synthetic_anomalies,\n",
                "    run_anomaly_detector,\n",
                "    evaluate_anomaly_detection,\n",
                "    run_anomaly_validation\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inject Synthetic Anomalies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = pd.read_parquet('data/processed/model_features.parquet')\n",
                "\n",
                "# Inject with 5% anomaly rate\n",
                "df_corrupted, ground_truth = inject_synthetic_anomalies(\n",
                "    features, \n",
                "    anomaly_rate=0.05,\n",
                "    spike_magnitude=3.0,\n",
                "    drop_magnitude=0.3\n",
                ")\n",
                "\n",
                "print(f\"Injected {ground_truth.sum()} synthetic anomalies out of {len(ground_truth)} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_cols = ['bio_update_child', 'demo_update_child', 'update_backlog_child', 'completion_rate_child']\n",
                "feature_cols = [c for c in feature_cols if c in df_corrupted.columns]\n",
                "print(f\"Using features: {feature_cols}\")\n",
                "\n",
                "predictions = run_anomaly_detector(df_corrupted, feature_cols)\n",
                "print(f\"Detected: {predictions.sum()} anomalies\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics = evaluate_anomaly_detection(ground_truth, predictions)\n",
                "\n",
                "print(\"ANOMALY DETECTION METRICS\")\n",
                "print(\"=\"*40)\n",
                "print(f\"Precision: {metrics['precision']:.3f}\")\n",
                "print(f\"Recall: {metrics['recall']:.3f}\")\n",
                "print(f\"F1-Score: {metrics['f1_score']:.3f}\")\n",
                "\n",
                "if metrics['precision'] >= 0.8:\n",
                "    print(\"\\n✅ PASS: Precision ≥ 0.8\")\n",
                "else:\n",
                "    print(f\"\\n⚠️ Precision {metrics['precision']:.2f} < 0.8 target\")\n",
                "    \n",
                "if metrics['recall'] >= 0.6:\n",
                "    print(\"✅ PASS: Recall ≥ 0.6\")\n",
                "else:\n",
                "    print(f\"⚠️ Recall {metrics['recall']:.2f} < 0.6 target\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Multi-Trial Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = run_anomaly_validation(\n",
                "    features_path='data/processed/model_features.parquet',\n",
                "    n_trials=5\n",
                ")\n",
                "\n",
                "if 'trial_results' in results and not results['trial_results'].empty:\n",
                "    fig = px.line(\n",
                "        results['trial_results'],\n",
                "        x='trial', y=['precision', 'recall', 'f1_score'],\n",
                "        title='Detection Metrics Across Trials'\n",
                "    )\n",
                "    fig.show()\n",
                "else:\n",
                "    print(\"No trial results to visualize\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}